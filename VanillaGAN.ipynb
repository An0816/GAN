{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VanillaGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15bxhU7P3rIARsU785h-_qLTZoSGiSMG7",
      "authorship_tag": "ABX9TyMiDkhIObgb6lf8mhEYHoaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/An0816/GAN/blob/main/VanillaGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfzkJrzq-WTk"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "import torch.utils.data as data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wrjzy16-k5Q"
      },
      "source": [
        "# custom data를 processing하기 위해, Dataset class를 상속받아서, 새로운 Dataset 클래스를 만듦\n",
        "# __init__, __len__, __getitem 선언\n",
        "# __init__(self, 매개변수) : 클래스 내에서 사용할 매개변수 정의 \n",
        "# __len__(self) : 데이터의 총 길이를 리턴\n",
        "# __getitem__(self, index) : dataset에서 특정 index에 있는 sample을 가져옴\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dataset = datasets.MNIST(root = 'MNIST/', \n",
        "                                      train = True, download = True, \n",
        "                                      transform = transforms.Compose([\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
        "                                                                                                                      \n",
        "        ]))\n",
        "    # train = True : create dataset from `MNIST/processed/training.pt`\n",
        "    # download = True : download the dataset from the internet and put it in root directory\n",
        "    # transform : 이미지 변형\n",
        "    # ToTensor() : numpy 배열의 이미지를 tensor 타입으로 변경해줌\n",
        "    # Normalize(mean, std, inplace = False) : 이미지를 정규화\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "# dataLoader : dataset을 인자로 받아서 data를 뽑아냄\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset):\n",
        "        super(data.DataLoader).__init__()\n",
        "\n",
        "        self.dataloader = data.DataLoader(dataset = dataset,\n",
        "                                          batch_size = 1,\n",
        "                                          shuffle = True,\n",
        "                                          drop_last = True)\n",
        "        \n",
        "# interpreter에서 직접 실행된 경우에만 if문 이하의 코드를 돌려라\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = Dataset()\n",
        "    dataLoader = DataLoader(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAt6Q4rlFAME"
      },
      "source": [
        "class Loss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.loss = torch.nn.BCELoss()\n",
        "    def forward(self, x, y):\n",
        "        return self.loss(x, y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.rand(3, 1)\n",
        "    y = torch.rand(3, 1)\n",
        "    loss = Loss()\n",
        "\n",
        "    print(loss(x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oB6TNhWFfln"
      },
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(100, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 784),\n",
        "            torch.nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjChxnaFGxGu"
      },
      "source": [
        "dataset = Dataset()\n",
        "data_loader = DataLoader(dataset)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "criterion = Loss()\n",
        "\n",
        "optim_G = torch.optim.Adam(G.parameters(), lr = 0.0001)\n",
        "optim_D = torch.optim.Adam(D.parameters(), lr = 0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXsl2n6Kcxa5"
      },
      "source": [
        "total_epochs = 200\n",
        "total_batch = len(data_loader.dataloader)\n",
        "\n",
        "for epcoh in range(total_epochs):\n",
        "    avg_cost = [0, 0]\n",
        "    for x, _ in data_loader.dataloader:\n",
        "        real = (torch.FloatTensor(x.size(0), 1).fill_(1.0)).to(device)\n",
        "        fake = (torch.FloatTensor(x.size(0), 1).fill_(0.0)).to(device)\n",
        "\n",
        "        x = x.view(x.size(0), -1).to(device)\n",
        "\n",
        "        noise = torch.randn(data_loader.dataloader.batch_size, 100, device = device)\n",
        "\n",
        "        fake_img = G(noise)\n",
        "\n",
        "        # train Generator\n",
        "        optim_G.zero_grad()\n",
        "        g_cost = criterion(D(fake_img), real)\n",
        "        g_cost.backward()\n",
        "        optim_G.step()\n",
        "\n",
        "        fake_img = fake_img.detach().to(device) # 얘를 굳이 하는 이유가 머야\n",
        "\n",
        "        # train Discriminator\n",
        "        optim_D.zero_grad()\n",
        "        d_cost = criterion(D(torch.cat((x, fake_img))), torch.cat((real, fake)))\n",
        "        d_cost.backward()\n",
        "        optim_D.step()\n",
        "\n",
        "        avg_cost[0] += g_cost\n",
        "        avg_cost[1] += d_cost\n",
        "\n",
        "    avg_cost[0] /= total_batch\n",
        "    avg_cost[1] /= total_batch\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        fake_img = fake_img.reshape([batch_size, 1, 28, 28])\n",
        "        img_grid = make_grid(fake_img, nrow=10, normalize=True)\n",
        "        save_image(img_grid, \"/content/drive/MyDrive/Deep Learning/GAN/GAN Result/%d.png\"%(epoch+1))\n",
        "        print(\"Epoch: %d, Generator: %f, Discriminator: %f\"%(epoch+1, avg_cost[0], avg_cost[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbk65TAPqTcV"
      },
      "source": [
        "    if (epcoh+1) % 10 == 0:\n",
        "        fake_img = fake_img.reshape([batch_size, 1, 28, 28])\n",
        "        img_grid = make_grid(fake_img, nrow = 10, normalize = True)\n",
        "        save_image(img_grid, \"/content/drive/MyDrive/Deep Learning/GAN/result/%d.png\"%(epoch + 1))\n",
        "        print(f\"Epoch : {epoch + 1},\\t Generator = {avg_cost[0]},\\t Discriminator = {avg_cost[1]}\")\n"
      ]
    }
  ]
}